{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **End-to-End Workflow for House Price Prediction Using Machine Learning**"
      ],
      "metadata": {
        "id": "GaQejTSBUZcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "html_code = '''\n",
        "<iframe\n",
        "    src=\"https://rmhalak-house-pricing-v1.hf.space/\"\n",
        "    frameborder=\"0\"\n",
        "    width=\"1200\"\n",
        "    height=\"750\"\n",
        "></iframe>\n",
        "'''\n",
        "\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "gAxZZ_qiU9qn",
        "outputId": "668af4d2-aed6-4dbb-de17-b7d686ef2fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe\n",
              "    src=\"https://rmhalak-house-pricing-v1.hf.space/\"\n",
              "    frameborder=\"0\"\n",
              "    width=\"1200\"\n",
              "    height=\"750\"\n",
              "></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "xoWXiH0qYXPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook demonstrates the end-to-end process of building, training, and deploying a machine learning model (fully-connected neural network) for predicting house prices. Here's a summary of the steps covered:\n",
        "\n",
        "1. **Load Data from Kaggle**\n",
        "   - The dataset is loaded into a Pandas DataFrame from a CSV file provided via Kaggle.\n",
        "\n",
        "2. **Import Libraries**\n",
        "   - Essential libraries are imported, including TensorFlow for building the model, Pandas and NumPy for data manipulation, and Matplotlib for visualization.\n",
        "\n",
        "3. **Preprocessing and Feature Generation**\n",
        "   - The dataset is preprocessed by dropping irrelevant columns, filtering out non-positive prices, and converting date columns.\n",
        "   - New features are created, such as `house_age` and `years_since_renovation`.\n",
        "   - Numerical features are bucketized and normalized.\n",
        "   - Categorical features are transformed into numerical format using one-hot encoding.\n",
        "\n",
        "4. **Generate Train and Test Sets**\n",
        "   - The dataset is split into training and testing sets using an 80/20 split.\n",
        "\n",
        "5. **Create the Model**\n",
        "   - A Sequential neural network model is defined with dense layers for regression tasks.\n",
        "   - The model is compiled with the Adam optimizer and Mean Squared Error loss function.\n",
        "   - An early stopping callback is used to prevent overfitting and restore the best weights.\n",
        "\n",
        "6. **Training**\n",
        "   - The model is trained on the training data for up to 1000 epochs with early stopping in place.\n",
        "   - A validation split is used to monitor performance during training.\n",
        "\n",
        "7. **Export the Trained Model**\n",
        "   - The trained model is saved to a file using `pickle`, allowing for future use without retraining.\n",
        "\n",
        "8. **Load the Trained Model**\n",
        "   - The saved model is loaded from the file for making predictions.\n",
        "\n",
        "9. **Prediction of a New Data Point using Trained Model**\n",
        "   - A new data point is prepared, processed, and used to make a prediction with the trained model.\n",
        "\n",
        "10. **Create a Web Application Based on the Trained Model**\n",
        "   - Deploy the trained model as a web application, allowing users to interact with it and make predictions via a user-friendly interface. This is implemented by:\n",
        "      * 10.1. Developing a web application using simple web interface frameworks like `streamlit`. This interface will allow users to input features such as bedrooms and bathrooms, and then use the trained model to predict house prices.\n",
        "      * 10.2. Deploying the application by hosting it on a cloud service such as Hugging Face Spaces or AWS. This makes the model accessible via a web browser.\n"
      ],
      "metadata": {
        "id": "zeY-OOfGSq5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data from Kaggle"
      ],
      "metadata": {
        "id": "z23KZK3I9SoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible to download datasets directly from Kaggle to your local environment or cloud storage using the Kaggle command-line tool or the Python kaggle package. This step typically involves specifying the dataset name and running the download command. Once the data is downloaded, it can be loaded for further processing and analysis."
      ],
      "metadata": {
        "id": "qOZakcHTMEIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d shree1992/housedata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtLe4QCoXmCM",
        "outputId": "5a5a6e6c-b593-4c76-c1a9-3bc359c51064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shree1992/housedata\n",
            "License(s): unknown\n",
            "Downloading housedata.zip to /content\n",
            "  0% 0.00/432k [00:00<?, ?B/s]\n",
            "100% 432k/432k [00:00<00:00, 28.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/housedata.zip'"
      ],
      "metadata": {
        "id": "632oMee7XqSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdce9fdd-c75e-4ae0-c268-60d9f560509c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/housedata.zip\n",
            "  inflating: data.csv                \n",
            "  inflating: data.dat                \n",
            "  inflating: output.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "qWGd-r-W9VOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next. the necessary libraries and packages are imported to facilitate data handling, machine learning, and model development.\n",
        "\n",
        "* `pandas` and `numpy` are used for data manipulation and numerical operations.\n",
        "* `tensorflow` is the core library used for building and training deep learning models, specifically using the Keras API for constructing neural networks.\n",
        "* `os` is used for interacting with the operating system, such as setting environment variables.\n",
        "* `pickle` and `json` are utilized for saving and loading serialized data structures.\n",
        "* `matplotlib.pyplot` is employed for creating visualizations, particularly for plotting data distributions and model performance.\n",
        "* `train_test_split` function from `sklearn.model_selection` is included to split the dataset into training and testing subsets. Additionally, a seed value of 42 is set to ensure reproducibility, meaning that the code will yield the same results each time it is run. Setting the `TF_DETERMINISTIC_OPS` environment variable ensures that TensorFlow operations behave deterministically, further contributing to consistent results across runs."
      ],
      "metadata": {
        "id": "ObxGLyhrMYhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Ensure reproducibility by configuring deterministic operations\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ],
      "metadata": {
        "id": "LAMD6bPeXvfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the Data"
      ],
      "metadata": {
        "id": "c2yoVao6Nj4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset is loaded into a Pandas DataFrame from a CSV file.\n",
        "df = pd.read_csv('/content/data.csv')"
      ],
      "metadata": {
        "id": "8LVw4ICaNWQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess the Data"
      ],
      "metadata": {
        "id": "e56Z4yO5Nezs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing begins by creating a copy of the original DataFrame.\n",
        "* Because this dataset concerns data taken only from Washington (USA), we drop irrelevant columns such as `country`, `street`, and `statezip`.\n",
        "* Rows with null or non-positive `price` are filtered out.\n",
        "* The `date` column is converted to a datetime object\n",
        "* The numeric columns like `bedrooms`, `bathrooms`, and `floors` are converted to integers for consistency"
      ],
      "metadata": {
        "id": "7wLJ1VtANrB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "df_features = df.copy()\n",
        "df_features = df_features.drop(columns=['country', 'street', 'statezip'])\n",
        "df_features = df_features[df_features['price'] > 0]\n",
        "df_features['date'] = pd.to_datetime(df_features['date'])\n",
        "for col in ['bedrooms', 'bathrooms', 'floors']:\n",
        "    df_features[col] = df_features[col].apply(lambda x:int(x))"
      ],
      "metadata": {
        "id": "SyuQI86uNmHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Features"
      ],
      "metadata": {
        "id": "hvYuRVxnNgKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "New features are generated to enrich the dataset. These include `house_age` (calculated as the difference between the year sold and the year built) and `years_since_renovation` (calculated similarly but considering renovations). A binary `has_basement` feature is created based on whether the house has a basement. Houses that haven't been renovated have their `yr_renovated` set to the year they were built.\n",
        "\n"
      ],
      "metadata": {
        "id": "wzFOcwMuOa-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_features(df):\n",
        "    df['year_sold'] = df['date'].dt.year\n",
        "    df = df.drop(columns=['date'])\n",
        "    df['house_age'] = df['year_sold'] - df['yr_built']\n",
        "    df['years_since_renovation'] = df['year_sold'] - df['yr_renovated']\n",
        "    df.drop(columns=['year_sold'], inplace=True)\n",
        "    df['has_basement'] = df['sqft_basement'].apply(lambda x: 1 if x > 0 else 0)\n",
        "    mask = df['yr_renovated'] == 0\n",
        "    df.loc[mask, 'yr_renovated'] = df.loc[mask, 'yr_built']\n",
        "    return df\n",
        "\n",
        "df_features = create_new_features(df_features)"
      ],
      "metadata": {
        "id": "2sX2ZfYkOk4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bucketize Numerical Features"
      ],
      "metadata": {
        "id": "GsuzXGqTOnIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify numerical features, bucketization is performed. For certain columns (e.g., `sqft_living`, `sqft_lot`), values are grouped into buckets of a specified size, making the data less granular and potentially more useful for our model."
      ],
      "metadata": {
        "id": "rmRhiQjJOpc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_sizes = {'sqft_living': 25,\n",
        "                'sqft_lot': 25,\n",
        "                'sqft_above': 25,\n",
        "                'sqft_basement': 25}\n",
        "\n",
        "def bucketize(df, col, size):\n",
        "    return df[col].apply(lambda x: (x // size)*size)\n",
        "\n",
        "for col, size in bucket_sizes.items():\n",
        "    df_features[col] = bucketize(df_features, col, size)"
      ],
      "metadata": {
        "id": "KNG-r_o1O1yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize Numerical Features"
      ],
      "metadata": {
        "id": "Jl040-0zOzeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization is applied to numerical features to scale them between 0 and 1, making the features comparable in magnitude and improving the performance of machine learning models. This is done using the minimum and maximum values of each numerical feature."
      ],
      "metadata": {
        "id": "gbCnVWOWO4rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront',\n",
        "                      'view', 'condition', 'sqft_above', 'sqft_basement',\n",
        "                      'yr_built', 'yr_renovated', 'house_age', 'years_since_renovation']\n",
        "\n",
        "min_dict = dict(df_features[numerical_features].min())\n",
        "max_dict = dict(df_features[numerical_features].max())\n",
        "\n",
        "def normalize(df, col):\n",
        "    df[col] = df[col].apply(lambda x: (x-min_dict[col])/(max_dict[col]-min_dict[col]))\n",
        "    return df[col]\n",
        "\n",
        "for col in numerical_features:\n",
        "    df_features[col] = normalize(df_features, col)"
      ],
      "metadata": {
        "id": "p5ne3k7DO-hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Categorical Features"
      ],
      "metadata": {
        "id": "5IP0wNItPBRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical features, such as `city`, are transformed into numerical features through one-hot encoding, where each unique category is converted into a new binary column. This process is essential for integrating categorical data into machine learning models that require numerical inputs."
      ],
      "metadata": {
        "id": "UjBmhwO2POke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features = ['city']\n",
        "for col in cat_features:\n",
        "    dummies = pd.get_dummies(df_features[col], prefix=col).apply(lambda x: x.astype(int))\n",
        "    df_features = pd.concat([df_features, dummies], axis=1)\n",
        "    df_features.drop(columns=[col], inplace=True)"
      ],
      "metadata": {
        "id": "HZ-viOc2xSbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Train and Test Sets"
      ],
      "metadata": {
        "id": "LnSyzbsH-AUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, the dataset is split into features `X` and target variable `y`. The target variable is price, which is separated from the rest of the features in the DataFrame.\n",
        "\n",
        "The dataset is then divided into training and testing sets using the `train_test_split` function from `sklearn.model_selection`. Here, 80% of the data is allocated to the training set (`X_train`, `y_train`), and the remaining 20% is reserved for testing (`X_test`, `y_test`). The `random_state` parameter is set to `42` to ensure the split is reproducible, meaning that the same split will occur every time the code is run."
      ],
      "metadata": {
        "id": "iFUQj37rPhk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate train and test sets\n",
        "X = df_features.drop(columns=['price'])\n",
        "y = df_features['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zdR9E9jcZYY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Model"
      ],
      "metadata": {
        "id": "0SXWCh1k-Cjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network model is created using TensorFlow's Keras API. The model is sequential, meaning layers are stacked in a linear fashion. The architecture consists of:\n",
        "\n",
        "* An input layer that matches the number of features in X_train.\n",
        "* Two hidden layers both using the ReLU activation function.\n",
        "* An output layer with a single neuron and no activation function, suitable for regression tasks where the model predicts a continuous value (in this case, the house price)."
      ],
      "metadata": {
        "id": "LVO3oVqLQBKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])"
      ],
      "metadata": {
        "id": "X8qNO-ne3tJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Stopping Callback"
      ],
      "metadata": {
        "id": "bw_wZyRhQVdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An early stopping callback is defined to prevent overfitting. It monitors the validation loss during training, and if the validation loss does not improve for 10 consecutive epochs, training is halted. The model will automatically restore the weights from the epoch with the best validation loss, ensuring the best model is retained."
      ],
      "metadata": {
        "id": "yJ1G5we5QUCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
        ")"
      ],
      "metadata": {
        "id": "q4p6AHb4QOZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the Model"
      ],
      "metadata": {
        "id": "qt9tPP1jQQdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is compiled with the `Adam` optimizer, a widely used optimizer in deep learning due to its efficiency and low memory requirements. The learning rate is set to `0.01`. The loss function used is Mean Squared Error (MSE), which is typical for regression problems. Additionally, Mean Absolute Error (MAE) is included as a metric to evaluate model performance during training."
      ],
      "metadata": {
        "id": "yli74mdTQaBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mse', 'mae'])"
      ],
      "metadata": {
        "id": "GsLjKPnlQSES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "yLESu0yC-ELa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained using the `fit` method. The training process involves:\n",
        "\n",
        "* `epochs` specifies the maximum number of epochs, or iterations over the entire dataset, for training. Training will stop earlier if early stopping criteria are met.\n",
        "* `batch_size` sets the number of samples per gradient update. Mini-batch training helps balance memory usage and training speed.\n",
        "* `validation_split` allocates 20% of the training data for validation during training. This validation set is used to monitor the model's performance and adjust training.\n",
        "* `callbacks=[early_stopping]` incorporates the early stopping callback defined earlier, which halts training if no improvement in validation loss is observed for 10 epochs and restores the best model weights.\n",
        "\n",
        "The `history` object returned by the fit method contains details about the training process, such as loss and metric values over epochs, which can be used to evaluate and visualize model performance."
      ],
      "metadata": {
        "id": "v1-GNJBqQmoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,\n",
        "                   y_train,\n",
        "                   epochs=1000,\n",
        "                   batch_size=64,\n",
        "                   validation_split=0.2,\n",
        "                   callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "Myt6wsc53wDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9d5a7a-19af-478c-e771-fa8594a2aa6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 674961358848.0000 - mae: 566886.9375 - mse: 674961358848.0000 - val_loss: 422528253952.0000 - val_mae: 547890.7500 - val_mse: 422528253952.0000\n",
            "Epoch 2/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 670222188544.0000 - mae: 562863.6250 - mse: 670222188544.0000 - val_loss: 402165071872.0000 - val_mae: 529796.5000 - val_mse: 402165071872.0000\n",
            "Epoch 3/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 637984636928.0000 - mae: 534731.4375 - mse: 637984636928.0000 - val_loss: 332867076096.0000 - val_mae: 463113.2812 - val_mse: 332867076096.0000\n",
            "Epoch 4/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 550546440192.0000 - mae: 449859.1250 - mse: 550546440192.0000 - val_loss: 216504090624.0000 - val_mae: 325685.2500 - val_mse: 216504090624.0000\n",
            "Epoch 5/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 426548854784.0000 - mae: 304012.6250 - mse: 426548854784.0000 - val_loss: 123343118336.0000 - val_mae: 212531.8750 - val_mse: 123343118336.0000\n",
            "Epoch 6/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 344381751296.0000 - mae: 217744.8125 - mse: 344381751296.0000 - val_loss: 99081961472.0000 - val_mae: 204040.9688 - val_mse: 99081961472.0000\n",
            "Epoch 7/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 325339545600.0000 - mae: 219285.3906 - mse: 325339545600.0000 - val_loss: 95742197760.0000 - val_mae: 206420.0938 - val_mse: 95742197760.0000\n",
            "Epoch 8/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 321754103808.0000 - mae: 219633.0000 - mse: 321754103808.0000 - val_loss: 93251944448.0000 - val_mae: 203497.3438 - val_mse: 93251944448.0000\n",
            "Epoch 9/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 319179718656.0000 - mae: 216309.3438 - mse: 319179718656.0000 - val_loss: 90976477184.0000 - val_mae: 200396.5000 - val_mse: 90976477184.0000\n",
            "Epoch 10/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 316842344448.0000 - mae: 213033.0156 - mse: 316842344448.0000 - val_loss: 88848654336.0000 - val_mae: 197544.4219 - val_mse: 88848654336.0000\n",
            "Epoch 11/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 314657767424.0000 - mae: 209889.5781 - mse: 314657767424.0000 - val_loss: 86822502400.0000 - val_mae: 194759.4219 - val_mse: 86822502400.0000\n",
            "Epoch 12/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 312586403840.0000 - mae: 206805.2969 - mse: 312586403840.0000 - val_loss: 84873748480.0000 - val_mae: 192043.0156 - val_mse: 84873748480.0000\n",
            "Epoch 13/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 310605086720.0000 - mae: 203788.0625 - mse: 310605086720.0000 - val_loss: 82987810816.0000 - val_mae: 189341.7812 - val_mse: 82987810816.0000\n",
            "Epoch 14/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 308699725824.0000 - mae: 200827.6719 - mse: 308699725824.0000 - val_loss: 81155653632.0000 - val_mae: 186687.2969 - val_mse: 81155653632.0000\n",
            "Epoch 15/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 306861735936.0000 - mae: 197960.1406 - mse: 306861735936.0000 - val_loss: 79371886592.0000 - val_mae: 184036.0625 - val_mse: 79371886592.0000\n",
            "Epoch 16/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 305086103552.0000 - mae: 195139.7969 - mse: 305086103552.0000 - val_loss: 77633576960.0000 - val_mae: 181404.9531 - val_mse: 77633576960.0000\n",
            "Epoch 17/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 303370174464.0000 - mae: 192419.0625 - mse: 303370174464.0000 - val_loss: 75939643392.0000 - val_mae: 178776.3906 - val_mse: 75939643392.0000\n",
            "Epoch 18/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 301713227776.0000 - mae: 189787.4844 - mse: 301713227776.0000 - val_loss: 74290298880.0000 - val_mae: 176138.4062 - val_mse: 74290298880.0000\n",
            "Epoch 19/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 300115689472.0000 - mae: 187217.4219 - mse: 300115689472.0000 - val_loss: 72686747648.0000 - val_mae: 173531.7500 - val_mse: 72686747648.0000\n",
            "Epoch 20/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 298578739200.0000 - mae: 184662.2031 - mse: 298578739200.0000 - val_loss: 71131234304.0000 - val_mae: 170952.2812 - val_mse: 71131234304.0000\n",
            "Epoch 21/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 297104769024.0000 - mae: 182163.6562 - mse: 297104769024.0000 - val_loss: 69626585088.0000 - val_mae: 168413.8438 - val_mse: 69626585088.0000\n",
            "Epoch 22/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 295695777792.0000 - mae: 179760.2344 - mse: 295695777792.0000 - val_loss: 68174573568.0000 - val_mae: 165983.8438 - val_mse: 68174573568.0000\n",
            "Epoch 23/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 294353469440.0000 - mae: 177467.5781 - mse: 294353469440.0000 - val_loss: 66776707072.0000 - val_mae: 163647.3750 - val_mse: 66776707072.0000\n",
            "Epoch 24/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 293078728704.0000 - mae: 175269.4375 - mse: 293078728704.0000 - val_loss: 65434599424.0000 - val_mae: 161389.9844 - val_mse: 65434599424.0000\n",
            "Epoch 25/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 291872473088.0000 - mae: 173182.3438 - mse: 291872473088.0000 - val_loss: 64149061632.0000 - val_mae: 159256.8438 - val_mse: 64149061632.0000\n",
            "Epoch 26/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 290734342144.0000 - mae: 171201.0156 - mse: 290734342144.0000 - val_loss: 62921621504.0000 - val_mae: 157240.2031 - val_mse: 62921621504.0000\n",
            "Epoch 27/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 289664303104.0000 - mae: 169355.1406 - mse: 289664303104.0000 - val_loss: 61751619584.0000 - val_mae: 155375.1719 - val_mse: 61751619584.0000\n",
            "Epoch 28/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 288660455424.0000 - mae: 167602.9844 - mse: 288660455424.0000 - val_loss: 60638228480.0000 - val_mae: 153663.7344 - val_mse: 60638228480.0000\n",
            "Epoch 29/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 287720079360.0000 - mae: 166005.5312 - mse: 287720079360.0000 - val_loss: 59580088320.0000 - val_mae: 152139.0000 - val_mse: 59580088320.0000\n",
            "Epoch 30/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 286840389632.0000 - mae: 164549.0469 - mse: 286840389632.0000 - val_loss: 58575495168.0000 - val_mae: 150709.6406 - val_mse: 58575495168.0000\n",
            "Epoch 31/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 286018306048.0000 - mae: 163235.7812 - mse: 286018306048.0000 - val_loss: 57622085632.0000 - val_mae: 149451.2031 - val_mse: 57622085632.0000\n",
            "Epoch 32/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 285249863680.0000 - mae: 162039.9219 - mse: 285249863680.0000 - val_loss: 56717189120.0000 - val_mae: 148289.8438 - val_mse: 56717189120.0000\n",
            "Epoch 33/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 284531326976.0000 - mae: 160935.7500 - mse: 284531326976.0000 - val_loss: 55858143232.0000 - val_mae: 147178.6406 - val_mse: 55858143232.0000\n",
            "Epoch 34/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 283858534400.0000 - mae: 159905.0781 - mse: 283858534400.0000 - val_loss: 55042404352.0000 - val_mae: 146134.9062 - val_mse: 55042404352.0000\n",
            "Epoch 35/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 283227848704.0000 - mae: 158934.5938 - mse: 283227848704.0000 - val_loss: 54267392000.0000 - val_mae: 145198.9844 - val_mse: 54267392000.0000\n",
            "Epoch 36/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 282635599872.0000 - mae: 158019.5781 - mse: 282635599872.0000 - val_loss: 53530177536.0000 - val_mae: 144343.7188 - val_mse: 53530177536.0000\n",
            "Epoch 37/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 282078150656.0000 - mae: 157158.7344 - mse: 282078150656.0000 - val_loss: 52828041216.0000 - val_mae: 143555.4375 - val_mse: 52828041216.0000\n",
            "Epoch 38/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 281552093184.0000 - mae: 156334.1094 - mse: 281552093184.0000 - val_loss: 52158484480.0000 - val_mae: 142851.3125 - val_mse: 52158484480.0000\n",
            "Epoch 39/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 281054380032.0000 - mae: 155555.6406 - mse: 281054380032.0000 - val_loss: 51519090688.0000 - val_mae: 142201.6719 - val_mse: 51519090688.0000\n",
            "Epoch 40/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 280582258688.0000 - mae: 154803.2500 - mse: 280582258688.0000 - val_loss: 50907852800.0000 - val_mae: 141595.7500 - val_mse: 50907852800.0000\n",
            "Epoch 41/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 280133435392.0000 - mae: 154086.2188 - mse: 280133435392.0000 - val_loss: 50322677760.0000 - val_mae: 141031.7344 - val_mse: 50322677760.0000\n",
            "Epoch 42/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 279705583616.0000 - mae: 153389.1875 - mse: 279705583616.0000 - val_loss: 49762701312.0000 - val_mae: 140474.1094 - val_mse: 49762701312.0000\n",
            "Epoch 43/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 279296966656.0000 - mae: 152724.1562 - mse: 279296966656.0000 - val_loss: 49225568256.0000 - val_mae: 139924.3594 - val_mse: 49225568256.0000\n",
            "Epoch 44/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 278905880576.0000 - mae: 152099.0469 - mse: 278905880576.0000 - val_loss: 48709783552.0000 - val_mae: 139401.7969 - val_mse: 48709783552.0000\n",
            "Epoch 45/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 278530621440.0000 - mae: 151509.5938 - mse: 278530621440.0000 - val_loss: 48213954560.0000 - val_mae: 138898.0625 - val_mse: 48213954560.0000\n",
            "Epoch 46/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 278169780224.0000 - mae: 150938.1562 - mse: 278169780224.0000 - val_loss: 47736360960.0000 - val_mae: 138412.3750 - val_mse: 47736360960.0000\n",
            "Epoch 47/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 277820342272.0000 - mae: 150382.8906 - mse: 277820342272.0000 - val_loss: 47267069952.0000 - val_mae: 137921.9375 - val_mse: 47267069952.0000\n",
            "Epoch 48/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 277471494144.0000 - mae: 149826.9375 - mse: 277471494144.0000 - val_loss: 46816313344.0000 - val_mae: 137463.4375 - val_mse: 46816313344.0000\n",
            "Epoch 49/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 277137457152.0000 - mae: 149303.2812 - mse: 277137457152.0000 - val_loss: 46381330432.0000 - val_mae: 137006.5312 - val_mse: 46381330432.0000\n",
            "Epoch 50/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 276815544320.0000 - mae: 148801.4375 - mse: 276815544320.0000 - val_loss: 45961793536.0000 - val_mae: 136572.7969 - val_mse: 45961793536.0000\n",
            "Epoch 51/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 276504510464.0000 - mae: 148317.1250 - mse: 276504510464.0000 - val_loss: 45554774016.0000 - val_mae: 136139.1094 - val_mse: 45554774016.0000\n",
            "Epoch 52/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 276202225664.0000 - mae: 147846.3750 - mse: 276202225664.0000 - val_loss: 45159538688.0000 - val_mae: 135709.4219 - val_mse: 45159538688.0000\n",
            "Epoch 53/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 275908231168.0000 - mae: 147398.8125 - mse: 275908231168.0000 - val_loss: 44776046592.0000 - val_mae: 135286.4844 - val_mse: 44776046592.0000\n",
            "Epoch 54/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 275622166528.0000 - mae: 146967.8438 - mse: 275622166528.0000 - val_loss: 44403859456.0000 - val_mae: 134864.9688 - val_mse: 44403859456.0000\n",
            "Epoch 55/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 275343376384.0000 - mae: 146547.1875 - mse: 275343376384.0000 - val_loss: 44042018816.0000 - val_mae: 134463.5781 - val_mse: 44042018816.0000\n",
            "Epoch 56/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 275071533056.0000 - mae: 146137.7031 - mse: 275071533056.0000 - val_loss: 43690364928.0000 - val_mae: 134080.4219 - val_mse: 43690364928.0000\n",
            "Epoch 57/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 274806734848.0000 - mae: 145739.6875 - mse: 274806734848.0000 - val_loss: 43348402176.0000 - val_mae: 133725.4219 - val_mse: 43348402176.0000\n",
            "Epoch 58/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 274548031488.0000 - mae: 145353.7031 - mse: 274548031488.0000 - val_loss: 43015507968.0000 - val_mae: 133390.6719 - val_mse: 43015507968.0000\n",
            "Epoch 59/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 274295422976.0000 - mae: 144977.5312 - mse: 274295422976.0000 - val_loss: 42690826240.0000 - val_mae: 133063.9844 - val_mse: 42690826240.0000\n",
            "Epoch 60/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 274048483328.0000 - mae: 144613.5156 - mse: 274048483328.0000 - val_loss: 42374287360.0000 - val_mae: 132745.1094 - val_mse: 42374287360.0000\n",
            "Epoch 61/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 273807163392.0000 - mae: 144255.8125 - mse: 273807163392.0000 - val_loss: 42065489920.0000 - val_mae: 132430.0625 - val_mse: 42065489920.0000\n",
            "Epoch 62/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 273571217408.0000 - mae: 143904.9688 - mse: 273571217408.0000 - val_loss: 41764601856.0000 - val_mae: 132127.4062 - val_mse: 41764601856.0000\n",
            "Epoch 63/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 273340776448.0000 - mae: 143567.8750 - mse: 273340776448.0000 - val_loss: 41471111168.0000 - val_mae: 131828.1875 - val_mse: 41471111168.0000\n",
            "Epoch 64/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 273115561984.0000 - mae: 143236.1562 - mse: 273115561984.0000 - val_loss: 41184788480.0000 - val_mae: 131536.2500 - val_mse: 41184788480.0000\n",
            "Epoch 65/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 272895262720.0000 - mae: 142912.6875 - mse: 272895262720.0000 - val_loss: 40905457664.0000 - val_mae: 131245.6094 - val_mse: 40905457664.0000\n",
            "Epoch 66/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 272679878656.0000 - mae: 142608.5938 - mse: 272679878656.0000 - val_loss: 40632811520.0000 - val_mae: 130956.6797 - val_mse: 40632811520.0000\n",
            "Epoch 67/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 272469278720.0000 - mae: 142317.8906 - mse: 272469278720.0000 - val_loss: 40366518272.0000 - val_mae: 130677.7031 - val_mse: 40366518272.0000\n",
            "Epoch 68/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 272263299072.0000 - mae: 142033.5781 - mse: 272263299072.0000 - val_loss: 40106659840.0000 - val_mae: 130409.4297 - val_mse: 40106659840.0000\n",
            "Epoch 69/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 272061956096.0000 - mae: 141760.7031 - mse: 272061956096.0000 - val_loss: 39852810240.0000 - val_mae: 130145.7344 - val_mse: 39852810240.0000\n",
            "Epoch 70/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 271865348096.0000 - mae: 141495.8594 - mse: 271865348096.0000 - val_loss: 39604936704.0000 - val_mae: 129882.4844 - val_mse: 39604936704.0000\n",
            "Epoch 71/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 271673114624.0000 - mae: 141236.9062 - mse: 271673114624.0000 - val_loss: 39362772992.0000 - val_mae: 129622.7578 - val_mse: 39362772992.0000\n",
            "Epoch 72/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 271484993536.0000 - mae: 140986.4375 - mse: 271484993536.0000 - val_loss: 39126511616.0000 - val_mae: 129371.1562 - val_mse: 39126511616.0000\n",
            "Epoch 73/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 271301279744.0000 - mae: 140742.8594 - mse: 271301279744.0000 - val_loss: 38896222208.0000 - val_mae: 129125.4375 - val_mse: 38896222208.0000\n",
            "Epoch 74/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 271121711104.0000 - mae: 140506.7344 - mse: 271121711104.0000 - val_loss: 38671409152.0000 - val_mae: 128880.8125 - val_mse: 38671409152.0000\n",
            "Epoch 75/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 270945976320.0000 - mae: 140276.0469 - mse: 270945976320.0000 - val_loss: 38451347456.0000 - val_mae: 128641.6172 - val_mse: 38451347456.0000\n",
            "Epoch 76/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 270773895168.0000 - mae: 140056.3750 - mse: 270773895168.0000 - val_loss: 38236958720.0000 - val_mae: 128420.8125 - val_mse: 38236958720.0000\n",
            "Epoch 77/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 270606041088.0000 - mae: 139844.1094 - mse: 270606041088.0000 - val_loss: 38027624448.0000 - val_mae: 128204.5312 - val_mse: 38027624448.0000\n",
            "Epoch 78/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 270441725952.0000 - mae: 139633.7344 - mse: 270441725952.0000 - val_loss: 37823287296.0000 - val_mae: 127994.2656 - val_mse: 37823287296.0000\n",
            "Epoch 79/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 270280769536.0000 - mae: 139433.9062 - mse: 270280769536.0000 - val_loss: 37624283136.0000 - val_mae: 127794.0859 - val_mse: 37624283136.0000\n",
            "Epoch 80/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 270123892736.0000 - mae: 139249.6562 - mse: 270123892736.0000 - val_loss: 37430603776.0000 - val_mae: 127611.7266 - val_mse: 37430603776.0000\n",
            "Epoch 81/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 269971013632.0000 - mae: 139080.1875 - mse: 269971013632.0000 - val_loss: 37241774080.0000 - val_mae: 127429.3984 - val_mse: 37241774080.0000\n",
            "Epoch 82/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 269821771776.0000 - mae: 138913.2188 - mse: 269821771776.0000 - val_loss: 37057163264.0000 - val_mae: 127245.8672 - val_mse: 37057163264.0000\n",
            "Epoch 83/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 269675839488.0000 - mae: 138749.4531 - mse: 269675839488.0000 - val_loss: 36877570048.0000 - val_mae: 127063.5469 - val_mse: 36877570048.0000\n",
            "Epoch 84/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 269533642752.0000 - mae: 138589.0625 - mse: 269533642752.0000 - val_loss: 36702691328.0000 - val_mae: 126885.3438 - val_mse: 36702691328.0000\n",
            "Epoch 85/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 269394673664.0000 - mae: 138427.6250 - mse: 269394673664.0000 - val_loss: 36533129216.0000 - val_mae: 126712.3281 - val_mse: 36533129216.0000\n",
            "Epoch 86/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 269259505664.0000 - mae: 138270.5938 - mse: 269259505664.0000 - val_loss: 36368011264.0000 - val_mae: 126540.9219 - val_mse: 36368011264.0000\n",
            "Epoch 87/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 269127303168.0000 - mae: 138115.4844 - mse: 269127303168.0000 - val_loss: 36207431680.0000 - val_mae: 126379.0078 - val_mse: 36207431680.0000\n",
            "Epoch 88/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 268998901760.0000 - mae: 137968.7969 - mse: 268998901760.0000 - val_loss: 36050550784.0000 - val_mae: 126224.1094 - val_mse: 36050550784.0000\n",
            "Epoch 89/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 268873662464.0000 - mae: 137822.0625 - mse: 268873662464.0000 - val_loss: 35897724928.0000 - val_mae: 126069.2344 - val_mse: 35897724928.0000\n",
            "Epoch 90/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 268751503360.0000 - mae: 137681.8281 - mse: 268751503360.0000 - val_loss: 35749142528.0000 - val_mae: 125915.4688 - val_mse: 35749142528.0000\n",
            "Epoch 91/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 268632883200.0000 - mae: 137544.0469 - mse: 268632883200.0000 - val_loss: 35604877312.0000 - val_mae: 125762.0547 - val_mse: 35604877312.0000\n",
            "Epoch 92/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 268517425152.0000 - mae: 137410.7031 - mse: 268517425152.0000 - val_loss: 35464622080.0000 - val_mae: 125610.1016 - val_mse: 35464622080.0000\n",
            "Epoch 93/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 268404981760.0000 - mae: 137284.3594 - mse: 268404981760.0000 - val_loss: 35328364544.0000 - val_mae: 125463.4609 - val_mse: 35328364544.0000\n",
            "Epoch 94/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 268295667712.0000 - mae: 137170.5938 - mse: 268295667712.0000 - val_loss: 35195441152.0000 - val_mae: 125316.7266 - val_mse: 35195441152.0000\n",
            "Epoch 95/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 268189089792.0000 - mae: 137063.6094 - mse: 268189089792.0000 - val_loss: 35066716160.0000 - val_mae: 125175.9219 - val_mse: 35066716160.0000\n",
            "Epoch 96/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 268085460992.0000 - mae: 136964.1250 - mse: 268085460992.0000 - val_loss: 34941030400.0000 - val_mae: 125037.3828 - val_mse: 34941030400.0000\n",
            "Epoch 97/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267984125952.0000 - mae: 136865.3438 - mse: 267984125952.0000 - val_loss: 34818871296.0000 - val_mae: 124902.7266 - val_mse: 34818871296.0000\n",
            "Epoch 98/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267885559808.0000 - mae: 136768.0781 - mse: 267885559808.0000 - val_loss: 34699534336.0000 - val_mae: 124783.3047 - val_mse: 34699534336.0000\n",
            "Epoch 99/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267789025280.0000 - mae: 136671.9688 - mse: 267789025280.0000 - val_loss: 34584285184.0000 - val_mae: 124674.6797 - val_mse: 34584285184.0000\n",
            "Epoch 100/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267695816704.0000 - mae: 136585.7656 - mse: 267695816704.0000 - val_loss: 34471952384.0000 - val_mae: 124567.6484 - val_mse: 34471952384.0000\n",
            "Epoch 101/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267603607552.0000 - mae: 136493.0156 - mse: 267603607552.0000 - val_loss: 34350069760.0000 - val_mae: 124416.2969 - val_mse: 34350069760.0000\n",
            "Epoch 102/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267501584384.0000 - mae: 136377.3750 - mse: 267501584384.0000 - val_loss: 34237648896.0000 - val_mae: 124296.5391 - val_mse: 34237648896.0000\n",
            "Epoch 103/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267407327232.0000 - mae: 136281.8281 - mse: 267407327232.0000 - val_loss: 34128216064.0000 - val_mae: 124182.3516 - val_mse: 34128216064.0000\n",
            "Epoch 104/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267315888128.0000 - mae: 136187.9062 - mse: 267315888128.0000 - val_loss: 34024099840.0000 - val_mae: 124083.0781 - val_mse: 34024099840.0000\n",
            "Epoch 105/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267227758592.0000 - mae: 136094.7031 - mse: 267227758592.0000 - val_loss: 33913153536.0000 - val_mae: 123947.6250 - val_mse: 33913153536.0000\n",
            "Epoch 106/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 267130585088.0000 - mae: 135984.3438 - mse: 267130585088.0000 - val_loss: 33809319936.0000 - val_mae: 123838.6250 - val_mse: 33809319936.0000\n",
            "Epoch 107/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 267040112640.0000 - mae: 135887.4688 - mse: 267040112640.0000 - val_loss: 33710866432.0000 - val_mae: 123743.7656 - val_mse: 33710866432.0000\n",
            "Epoch 108/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266954588160.0000 - mae: 135803.2500 - mse: 266954588160.0000 - val_loss: 33615716352.0000 - val_mae: 123650.0781 - val_mse: 33615716352.0000\n",
            "Epoch 109/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266872045568.0000 - mae: 135719.9062 - mse: 266872045568.0000 - val_loss: 33524781056.0000 - val_mae: 123560.5625 - val_mse: 33524781056.0000\n",
            "Epoch 110/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266791731200.0000 - mae: 135638.8594 - mse: 266791731200.0000 - val_loss: 33435822080.0000 - val_mae: 123470.6172 - val_mse: 33435822080.0000\n",
            "Epoch 111/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266713104384.0000 - mae: 135562.6094 - mse: 266713104384.0000 - val_loss: 33349259264.0000 - val_mae: 123381.2344 - val_mse: 33349259264.0000\n",
            "Epoch 112/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266636296192.0000 - mae: 135491.6406 - mse: 266636296192.0000 - val_loss: 33264783360.0000 - val_mae: 123293.1875 - val_mse: 33264783360.0000\n",
            "Epoch 113/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266561470464.0000 - mae: 135420.2969 - mse: 266561470464.0000 - val_loss: 33181806592.0000 - val_mae: 123207.7500 - val_mse: 33181806592.0000\n",
            "Epoch 114/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266487939072.0000 - mae: 135349.2656 - mse: 266487939072.0000 - val_loss: 33101406208.0000 - val_mae: 123125.9766 - val_mse: 33101406208.0000\n",
            "Epoch 115/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266416029696.0000 - mae: 135278.8906 - mse: 266416029696.0000 - val_loss: 33021962240.0000 - val_mae: 123039.4062 - val_mse: 33021962240.0000\n",
            "Epoch 116/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266345086976.0000 - mae: 135205.3125 - mse: 266345086976.0000 - val_loss: 32945727488.0000 - val_mae: 122961.5312 - val_mse: 32945727488.0000\n",
            "Epoch 117/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266276159488.0000 - mae: 135134.8906 - mse: 266276159488.0000 - val_loss: 32869447680.0000 - val_mae: 122876.4688 - val_mse: 32869447680.0000\n",
            "Epoch 118/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266207641600.0000 - mae: 135064.2188 - mse: 266207641600.0000 - val_loss: 32795113472.0000 - val_mae: 122795.9688 - val_mse: 32795113472.0000\n",
            "Epoch 119/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266140876800.0000 - mae: 134995.0938 - mse: 266140876800.0000 - val_loss: 32722622464.0000 - val_mae: 122725.0625 - val_mse: 32722622464.0000\n",
            "Epoch 120/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266075111424.0000 - mae: 134927.2500 - mse: 266075111424.0000 - val_loss: 32649320448.0000 - val_mae: 122652.0625 - val_mse: 32649320448.0000\n",
            "Epoch 121/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 266009772032.0000 - mae: 134856.8438 - mse: 266009772032.0000 - val_loss: 32578250752.0000 - val_mae: 122580.6016 - val_mse: 32578250752.0000\n",
            "Epoch 122/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265945595904.0000 - mae: 134783.2656 - mse: 265945595904.0000 - val_loss: 32509419520.0000 - val_mae: 122510.4375 - val_mse: 32509419520.0000\n",
            "Epoch 123/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265882468352.0000 - mae: 134710.2031 - mse: 265882468352.0000 - val_loss: 32442195968.0000 - val_mae: 122440.0547 - val_mse: 32442195968.0000\n",
            "Epoch 124/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 265819783168.0000 - mae: 134637.1875 - mse: 265819783168.0000 - val_loss: 32375095296.0000 - val_mae: 122363.9375 - val_mse: 32375095296.0000\n",
            "Epoch 125/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 265757720576.0000 - mae: 134562.9219 - mse: 265757720576.0000 - val_loss: 32310198272.0000 - val_mae: 122293.2734 - val_mse: 32310198272.0000\n",
            "Epoch 126/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 265697132544.0000 - mae: 134492.6250 - mse: 265697132544.0000 - val_loss: 32246480896.0000 - val_mae: 122221.2188 - val_mse: 32246480896.0000\n",
            "Epoch 127/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 265636888576.0000 - mae: 134424.0000 - mse: 265636888576.0000 - val_loss: 32183494656.0000 - val_mae: 122147.5625 - val_mse: 32183494656.0000\n",
            "Epoch 128/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265577709568.0000 - mae: 134359.5781 - mse: 265577709568.0000 - val_loss: 32122382336.0000 - val_mae: 122076.8125 - val_mse: 32122382336.0000\n",
            "Epoch 129/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 265520021504.0000 - mae: 134299.1250 - mse: 265520021504.0000 - val_loss: 32063262720.0000 - val_mae: 122015.3594 - val_mse: 32063262720.0000\n",
            "Epoch 130/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265462579200.0000 - mae: 134235.7344 - mse: 265462579200.0000 - val_loss: 31988672512.0000 - val_mae: 121891.2109 - val_mse: 31988672512.0000\n",
            "Epoch 131/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265392930816.0000 - mae: 134142.6094 - mse: 265392930816.0000 - val_loss: 31919962112.0000 - val_mae: 121787.1875 - val_mse: 31919962112.0000\n",
            "Epoch 132/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265328066560.0000 - mae: 134059.1406 - mse: 265328066560.0000 - val_loss: 31852736512.0000 - val_mae: 121686.7344 - val_mse: 31852736512.0000\n",
            "Epoch 133/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 265264676864.0000 - mae: 133981.0781 - mse: 265264676864.0000 - val_loss: 31791011840.0000 - val_mae: 121603.2109 - val_mse: 31791011840.0000\n",
            "Epoch 134/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 265204203520.0000 - mae: 133908.8750 - mse: 265204203520.0000 - val_loss: 31734335488.0000 - val_mae: 121529.9453 - val_mse: 31734335488.0000\n",
            "Epoch 135/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 265147088896.0000 - mae: 133848.0469 - mse: 265147088896.0000 - val_loss: 31678474240.0000 - val_mae: 121455.8047 - val_mse: 31678474240.0000\n",
            "Epoch 136/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 265091284992.0000 - mae: 133787.9688 - mse: 265091284992.0000 - val_loss: 31624118272.0000 - val_mae: 121381.9688 - val_mse: 31624118272.0000\n",
            "Epoch 137/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 265036513280.0000 - mae: 133733.3594 - mse: 265036513280.0000 - val_loss: 31570092032.0000 - val_mae: 121305.1875 - val_mse: 31570092032.0000\n",
            "Epoch 138/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264982626304.0000 - mae: 133675.0156 - mse: 264982626304.0000 - val_loss: 31519182848.0000 - val_mae: 121235.1094 - val_mse: 31519182848.0000\n",
            "Epoch 139/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264930197504.0000 - mae: 133620.2188 - mse: 264930197504.0000 - val_loss: 31464626176.0000 - val_mae: 121149.6719 - val_mse: 31464626176.0000\n",
            "Epoch 140/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264876015616.0000 - mae: 133554.3438 - mse: 264876015616.0000 - val_loss: 31411666944.0000 - val_mae: 121068.3828 - val_mse: 31411666944.0000\n",
            "Epoch 141/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264820473856.0000 - mae: 133482.8594 - mse: 264820473856.0000 - val_loss: 31350020096.0000 - val_mae: 120955.8672 - val_mse: 31350020096.0000\n",
            "Epoch 142/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264757460992.0000 - mae: 133395.3281 - mse: 264757460992.0000 - val_loss: 31293609984.0000 - val_mae: 120861.6172 - val_mse: 31293609984.0000\n",
            "Epoch 143/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264698576896.0000 - mae: 133317.1250 - mse: 264698576896.0000 - val_loss: 31239174144.0000 - val_mae: 120771.7031 - val_mse: 31239174144.0000\n",
            "Epoch 144/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264641298432.0000 - mae: 133245.1094 - mse: 264641298432.0000 - val_loss: 31187064832.0000 - val_mae: 120687.3047 - val_mse: 31187064832.0000\n",
            "Epoch 145/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264585904128.0000 - mae: 133176.6875 - mse: 264585904128.0000 - val_loss: 31138037760.0000 - val_mae: 120614.6484 - val_mse: 31138037760.0000\n",
            "Epoch 146/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264532557824.0000 - mae: 133111.8125 - mse: 264532557824.0000 - val_loss: 31088869376.0000 - val_mae: 120539.2188 - val_mse: 31088869376.0000\n",
            "Epoch 147/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264479096832.0000 - mae: 133043.4844 - mse: 264479096832.0000 - val_loss: 31039129600.0000 - val_mae: 120457.4297 - val_mse: 31039129600.0000\n",
            "Epoch 148/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264425275392.0000 - mae: 132970.5156 - mse: 264425275392.0000 - val_loss: 30991380480.0000 - val_mae: 120382.2344 - val_mse: 30991380480.0000\n",
            "Epoch 149/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264372322304.0000 - mae: 132897.2344 - mse: 264372322304.0000 - val_loss: 30943309824.0000 - val_mae: 120302.1641 - val_mse: 30943309824.0000\n",
            "Epoch 150/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264319336448.0000 - mae: 132825.5156 - mse: 264319336448.0000 - val_loss: 30897340416.0000 - val_mae: 120229.0781 - val_mse: 30897340416.0000\n",
            "Epoch 151/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264267448320.0000 - mae: 132753.6250 - mse: 264267448320.0000 - val_loss: 30848296960.0000 - val_mae: 120138.6797 - val_mse: 30848296960.0000\n",
            "Epoch 152/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 264214298624.0000 - mae: 132676.0469 - mse: 264214298624.0000 - val_loss: 30801397760.0000 - val_mae: 120053.3047 - val_mse: 30801397760.0000\n",
            "Epoch 153/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264162279424.0000 - mae: 132601.7031 - mse: 264162279424.0000 - val_loss: 30754629632.0000 - val_mae: 119966.7656 - val_mse: 30754629632.0000\n",
            "Epoch 154/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264110030848.0000 - mae: 132527.5469 - mse: 264110030848.0000 - val_loss: 30707302400.0000 - val_mae: 119879.0000 - val_mse: 30707302400.0000\n",
            "Epoch 155/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264058093568.0000 - mae: 132453.2969 - mse: 264058093568.0000 - val_loss: 30662465536.0000 - val_mae: 119801.0078 - val_mse: 30662465536.0000\n",
            "Epoch 156/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 264007122944.0000 - mae: 132383.2969 - mse: 264007122944.0000 - val_loss: 30615328768.0000 - val_mae: 119712.6953 - val_mse: 30615328768.0000\n",
            "Epoch 157/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263955709952.0000 - mae: 132306.7344 - mse: 263955709952.0000 - val_loss: 30569560064.0000 - val_mae: 119623.1641 - val_mse: 30569560064.0000\n",
            "Epoch 158/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 263904526336.0000 - mae: 132229.3125 - mse: 263904526336.0000 - val_loss: 30523148288.0000 - val_mae: 119530.9453 - val_mse: 30523148288.0000\n",
            "Epoch 159/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 263854194688.0000 - mae: 132151.4375 - mse: 263854194688.0000 - val_loss: 30478290944.0000 - val_mae: 119440.8906 - val_mse: 30478290944.0000\n",
            "Epoch 160/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 263804534784.0000 - mae: 132077.2969 - mse: 263804534784.0000 - val_loss: 30432905216.0000 - val_mae: 119346.4922 - val_mse: 30432905216.0000\n",
            "Epoch 161/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263754727424.0000 - mae: 131997.0000 - mse: 263754727424.0000 - val_loss: 30388672512.0000 - val_mae: 119253.6484 - val_mse: 30388672512.0000\n",
            "Epoch 162/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263705231360.0000 - mae: 131916.3750 - mse: 263705231360.0000 - val_loss: 30343081984.0000 - val_mae: 119152.8047 - val_mse: 30343081984.0000\n",
            "Epoch 163/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263655522304.0000 - mae: 131832.8438 - mse: 263655522304.0000 - val_loss: 30300692480.0000 - val_mae: 119061.7344 - val_mse: 30300692480.0000\n",
            "Epoch 164/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263607222272.0000 - mae: 131752.5000 - mse: 263607222272.0000 - val_loss: 30256697344.0000 - val_mae: 118960.3828 - val_mse: 30256697344.0000\n",
            "Epoch 165/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263558299648.0000 - mae: 131666.4844 - mse: 263558299648.0000 - val_loss: 30213851136.0000 - val_mae: 118861.9375 - val_mse: 30213851136.0000\n",
            "Epoch 166/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263510097920.0000 - mae: 131583.8438 - mse: 263510097920.0000 - val_loss: 30171725824.0000 - val_mae: 118767.9453 - val_mse: 30171725824.0000\n",
            "Epoch 167/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263462256640.0000 - mae: 131503.1250 - mse: 263462256640.0000 - val_loss: 30129965056.0000 - val_mae: 118671.2422 - val_mse: 30129965056.0000\n",
            "Epoch 168/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263414382592.0000 - mae: 131423.8906 - mse: 263414382592.0000 - val_loss: 30088734720.0000 - val_mae: 118575.6250 - val_mse: 30088734720.0000\n",
            "Epoch 169/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263367344128.0000 - mae: 131343.3438 - mse: 263367344128.0000 - val_loss: 30046676992.0000 - val_mae: 118475.5391 - val_mse: 30046676992.0000\n",
            "Epoch 170/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263320322048.0000 - mae: 131261.7344 - mse: 263320322048.0000 - val_loss: 30005157888.0000 - val_mae: 118375.3516 - val_mse: 30005157888.0000\n",
            "Epoch 171/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 263273381888.0000 - mae: 131178.9688 - mse: 263273381888.0000 - val_loss: 29964693504.0000 - val_mae: 118279.7344 - val_mse: 29964693504.0000\n",
            "Epoch 172/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 263227080704.0000 - mae: 131097.1562 - mse: 263227080704.0000 - val_loss: 29921568768.0000 - val_mae: 118174.9375 - val_mse: 29921568768.0000\n",
            "Epoch 173/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 263179632640.0000 - mae: 131010.8750 - mse: 263179632640.0000 - val_loss: 29882058752.0000 - val_mae: 118082.0000 - val_mse: 29882058752.0000\n",
            "Epoch 174/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 263133528064.0000 - mae: 130930.0547 - mse: 263133528064.0000 - val_loss: 29840869376.0000 - val_mae: 117983.5156 - val_mse: 29840869376.0000\n",
            "Epoch 175/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 263087767552.0000 - mae: 130849.5938 - mse: 263087767552.0000 - val_loss: 29800804352.0000 - val_mae: 117886.9219 - val_mse: 29800804352.0000\n",
            "Epoch 176/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 263042531328.0000 - mae: 130770.2969 - mse: 263042531328.0000 - val_loss: 29762488320.0000 - val_mae: 117795.7031 - val_mse: 29762488320.0000\n",
            "Epoch 177/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262997409792.0000 - mae: 130692.6484 - mse: 262997409792.0000 - val_loss: 29724643328.0000 - val_mae: 117707.0547 - val_mse: 29724643328.0000\n",
            "Epoch 178/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262953254912.0000 - mae: 130617.2109 - mse: 262953254912.0000 - val_loss: 29685538816.0000 - val_mae: 117610.5156 - val_mse: 29685538816.0000\n",
            "Epoch 179/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262908428288.0000 - mae: 130540.7969 - mse: 262908428288.0000 - val_loss: 29649610752.0000 - val_mae: 117523.5156 - val_mse: 29649610752.0000\n",
            "Epoch 180/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 262865059840.0000 - mae: 130465.9219 - mse: 262865059840.0000 - val_loss: 29613807616.0000 - val_mae: 117435.2188 - val_mse: 29613807616.0000\n",
            "Epoch 181/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 262821101568.0000 - mae: 130391.4844 - mse: 262821101568.0000 - val_loss: 29578557440.0000 - val_mae: 117347.3438 - val_mse: 29578557440.0000\n",
            "Epoch 182/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262776537088.0000 - mae: 130309.7422 - mse: 262776537088.0000 - val_loss: 29533597696.0000 - val_mae: 117231.2109 - val_mse: 29533597696.0000\n",
            "Epoch 183/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262724714496.0000 - mae: 130212.8984 - mse: 262724714496.0000 - val_loss: 29495508992.0000 - val_mae: 117135.3281 - val_mse: 29495508992.0000\n",
            "Epoch 184/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262677135360.0000 - mae: 130130.0234 - mse: 262677135360.0000 - val_loss: 29457119232.0000 - val_mae: 117037.1875 - val_mse: 29457119232.0000\n",
            "Epoch 185/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262630604800.0000 - mae: 130047.9609 - mse: 262630604800.0000 - val_loss: 29420812288.0000 - val_mae: 116944.1328 - val_mse: 29420812288.0000\n",
            "Epoch 186/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262584991744.0000 - mae: 129971.0938 - mse: 262584991744.0000 - val_loss: 29385703424.0000 - val_mae: 116855.0859 - val_mse: 29385703424.0000\n",
            "Epoch 187/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262540722176.0000 - mae: 129895.9531 - mse: 262540722176.0000 - val_loss: 29351499776.0000 - val_mae: 116765.6484 - val_mse: 29351499776.0000\n",
            "Epoch 188/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262497091584.0000 - mae: 129823.6328 - mse: 262497091584.0000 - val_loss: 29316968448.0000 - val_mae: 116673.2734 - val_mse: 29316968448.0000\n",
            "Epoch 189/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262453493760.0000 - mae: 129749.9922 - mse: 262453493760.0000 - val_loss: 29282899968.0000 - val_mae: 116583.6562 - val_mse: 29282899968.0000\n",
            "Epoch 190/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262410534912.0000 - mae: 129677.0156 - mse: 262410534912.0000 - val_loss: 29249480704.0000 - val_mae: 116492.8359 - val_mse: 29249480704.0000\n",
            "Epoch 191/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262367920128.0000 - mae: 129606.0234 - mse: 262367920128.0000 - val_loss: 29217435648.0000 - val_mae: 116405.7812 - val_mse: 29217435648.0000\n",
            "Epoch 192/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262327025664.0000 - mae: 129539.3906 - mse: 262327025664.0000 - val_loss: 29184937984.0000 - val_mae: 116314.1016 - val_mse: 29184937984.0000\n",
            "Epoch 193/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262285524992.0000 - mae: 129472.1250 - mse: 262285524992.0000 - val_loss: 29154322432.0000 - val_mae: 116227.0625 - val_mse: 29154322432.0000\n",
            "Epoch 194/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262245810176.0000 - mae: 129407.2812 - mse: 262245810176.0000 - val_loss: 29122598912.0000 - val_mae: 116135.3516 - val_mse: 29122598912.0000\n",
            "Epoch 195/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262204784640.0000 - mae: 129339.7188 - mse: 262204784640.0000 - val_loss: 29091670016.0000 - val_mae: 116049.2188 - val_mse: 29091670016.0000\n",
            "Epoch 196/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 262164627456.0000 - mae: 129273.3672 - mse: 262164627456.0000 - val_loss: 29062920192.0000 - val_mae: 115965.3047 - val_mse: 29062920192.0000\n",
            "Epoch 197/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 262125961216.0000 - mae: 129211.7734 - mse: 262125961216.0000 - val_loss: 29034530816.0000 - val_mae: 115880.2109 - val_mse: 29034530816.0000\n",
            "Epoch 198/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 262086950912.0000 - mae: 129148.4844 - mse: 262086950912.0000 - val_loss: 29005142016.0000 - val_mae: 115791.9766 - val_mse: 29005142016.0000\n",
            "Epoch 199/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262048956416.0000 - mae: 129085.5469 - mse: 262048956416.0000 - val_loss: 28977498112.0000 - val_mae: 115709.5391 - val_mse: 28977498112.0000\n",
            "Epoch 200/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 262011486208.0000 - mae: 129024.4922 - mse: 262011486208.0000 - val_loss: 28947470336.0000 - val_mae: 115617.8047 - val_mse: 28947470336.0000\n",
            "Epoch 201/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261973311488.0000 - mae: 128958.9141 - mse: 261973311488.0000 - val_loss: 28922013696.0000 - val_mae: 115538.2422 - val_mse: 28922015744.0000\n",
            "Epoch 202/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261937496064.0000 - mae: 128900.1406 - mse: 261937496064.0000 - val_loss: 28892819456.0000 - val_mae: 115448.2734 - val_mse: 28892819456.0000\n",
            "Epoch 203/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261900730368.0000 - mae: 128835.6875 - mse: 261900730368.0000 - val_loss: 28867121152.0000 - val_mae: 115368.4688 - val_mse: 28867121152.0000\n",
            "Epoch 204/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261865799680.0000 - mae: 128778.0234 - mse: 261865799680.0000 - val_loss: 28841371648.0000 - val_mae: 115288.5625 - val_mse: 28841371648.0000\n",
            "Epoch 205/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261830279168.0000 - mae: 128720.7344 - mse: 261830279168.0000 - val_loss: 28818870272.0000 - val_mae: 115218.2500 - val_mse: 28818870272.0000\n",
            "Epoch 206/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261796708352.0000 - mae: 128666.7891 - mse: 261796708352.0000 - val_loss: 28793430016.0000 - val_mae: 115135.1953 - val_mse: 28793430016.0000\n",
            "Epoch 207/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261761925120.0000 - mae: 128613.3828 - mse: 261761925120.0000 - val_loss: 28770785280.0000 - val_mae: 115064.7656 - val_mse: 28770785280.0000\n",
            "Epoch 208/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261728763904.0000 - mae: 128561.2812 - mse: 261728763904.0000 - val_loss: 28744941568.0000 - val_mae: 114980.6172 - val_mse: 28744941568.0000\n",
            "Epoch 209/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261694586880.0000 - mae: 128504.3438 - mse: 261694586880.0000 - val_loss: 28725102592.0000 - val_mae: 114912.8047 - val_mse: 28725102592.0000\n",
            "Epoch 210/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261662474240.0000 - mae: 128453.9219 - mse: 261662474240.0000 - val_loss: 28703604736.0000 - val_mae: 114840.3438 - val_mse: 28703604736.0000\n",
            "Epoch 211/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261629902848.0000 - mae: 128403.6016 - mse: 261629902848.0000 - val_loss: 28683395072.0000 - val_mae: 114771.2109 - val_mse: 28683395072.0000\n",
            "Epoch 212/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261597757440.0000 - mae: 128353.6641 - mse: 261597757440.0000 - val_loss: 28660680704.0000 - val_mae: 114691.9453 - val_mse: 28660680704.0000\n",
            "Epoch 213/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 261564809216.0000 - mae: 128301.9531 - mse: 261564809216.0000 - val_loss: 28639520768.0000 - val_mae: 114619.3516 - val_mse: 28639520768.0000\n",
            "Epoch 214/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261532860416.0000 - mae: 128253.8516 - mse: 261532860416.0000 - val_loss: 28621170688.0000 - val_mae: 114556.5625 - val_mse: 28621170688.0000\n",
            "Epoch 215/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261502533632.0000 - mae: 128210.0078 - mse: 261502533632.0000 - val_loss: 28600866816.0000 - val_mae: 114485.3203 - val_mse: 28600866816.0000\n",
            "Epoch 216/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261471485952.0000 - mae: 128160.9297 - mse: 261471485952.0000 - val_loss: 28580106240.0000 - val_mae: 114413.3828 - val_mse: 28580106240.0000\n",
            "Epoch 217/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261440290816.0000 - mae: 128111.8828 - mse: 261440290816.0000 - val_loss: 28562755584.0000 - val_mae: 114351.5703 - val_mse: 28562755584.0000\n",
            "Epoch 218/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261410963456.0000 - mae: 128071.0781 - mse: 261410963456.0000 - val_loss: 28544641024.0000 - val_mae: 114288.1875 - val_mse: 28544641024.0000\n",
            "Epoch 219/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261381341184.0000 - mae: 128028.7422 - mse: 261381341184.0000 - val_loss: 28526227456.0000 - val_mae: 114220.2109 - val_mse: 28526227456.0000\n",
            "Epoch 220/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261351882752.0000 - mae: 127987.4297 - mse: 261351882752.0000 - val_loss: 28511737856.0000 - val_mae: 114165.5625 - val_mse: 28511737856.0000\n",
            "Epoch 221/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 261323800576.0000 - mae: 127950.6172 - mse: 261323800576.0000 - val_loss: 28493819904.0000 - val_mae: 114095.4531 - val_mse: 28493819904.0000\n",
            "Epoch 222/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 261293965312.0000 - mae: 127909.4453 - mse: 261293965312.0000 - val_loss: 28477427712.0000 - val_mae: 114036.5703 - val_mse: 28477427712.0000\n",
            "Epoch 223/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 261265817600.0000 - mae: 127873.2188 - mse: 261265817600.0000 - val_loss: 28460646400.0000 - val_mae: 113975.4922 - val_mse: 28460646400.0000\n",
            "Epoch 224/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261236965376.0000 - mae: 127834.1328 - mse: 261236965376.0000 - val_loss: 28445669376.0000 - val_mae: 113921.3203 - val_mse: 28445669376.0000\n",
            "Epoch 225/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261209505792.0000 - mae: 127799.7109 - mse: 261209505792.0000 - val_loss: 28430188544.0000 - val_mae: 113865.7891 - val_mse: 28430188544.0000\n",
            "Epoch 226/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261181128704.0000 - mae: 127760.6094 - mse: 261181128704.0000 - val_loss: 28415160320.0000 - val_mae: 113814.8125 - val_mse: 28415160320.0000\n",
            "Epoch 227/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261153832960.0000 - mae: 127722.9375 - mse: 261153832960.0000 - val_loss: 28396181504.0000 - val_mae: 113750.5859 - val_mse: 28396181504.0000\n",
            "Epoch 228/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261124898816.0000 - mae: 127681.0156 - mse: 261124898816.0000 - val_loss: 28383664128.0000 - val_mae: 113705.9219 - val_mse: 28383664128.0000\n",
            "Epoch 229/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261098946560.0000 - mae: 127648.0391 - mse: 261098946560.0000 - val_loss: 28369612800.0000 - val_mae: 113654.5703 - val_mse: 28369612800.0000\n",
            "Epoch 230/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 261072519168.0000 - mae: 127612.3672 - mse: 261072519168.0000 - val_loss: 28356151296.0000 - val_mae: 113605.2734 - val_mse: 28356151296.0000\n",
            "Epoch 231/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 261045993472.0000 - mae: 127577.9141 - mse: 261045993472.0000 - val_loss: 28341479424.0000 - val_mae: 113552.6562 - val_mse: 28341479424.0000\n",
            "Epoch 232/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 261019713536.0000 - mae: 127541.1562 - mse: 261019713536.0000 - val_loss: 28324077568.0000 - val_mae: 113492.7578 - val_mse: 28324077568.0000\n",
            "Epoch 233/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260992909312.0000 - mae: 127499.6719 - mse: 260992909312.0000 - val_loss: 28311603200.0000 - val_mae: 113439.5312 - val_mse: 28311603200.0000\n",
            "Epoch 234/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260967972864.0000 - mae: 127466.1328 - mse: 260967972864.0000 - val_loss: 28299231232.0000 - val_mae: 113390.8125 - val_mse: 28299231232.0000\n",
            "Epoch 235/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260942790656.0000 - mae: 127432.0938 - mse: 260942790656.0000 - val_loss: 28286195712.0000 - val_mae: 113339.1641 - val_mse: 28286195712.0000\n",
            "Epoch 236/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260917772288.0000 - mae: 127397.9766 - mse: 260917772288.0000 - val_loss: 28273096704.0000 - val_mae: 113289.8125 - val_mse: 28273096704.0000\n",
            "Epoch 237/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260893294592.0000 - mae: 127364.9062 - mse: 260893294592.0000 - val_loss: 28258500608.0000 - val_mae: 113233.6797 - val_mse: 28258500608.0000\n",
            "Epoch 238/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260867293184.0000 - mae: 127326.8281 - mse: 260867293184.0000 - val_loss: 28245788672.0000 - val_mae: 113184.4922 - val_mse: 28245788672.0000\n",
            "Epoch 239/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260843225088.0000 - mae: 127294.1797 - mse: 260843225088.0000 - val_loss: 28233375744.0000 - val_mae: 113135.6406 - val_mse: 28233375744.0000\n",
            "Epoch 240/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260818370560.0000 - mae: 127259.5312 - mse: 260818370560.0000 - val_loss: 28219758592.0000 - val_mae: 113082.7031 - val_mse: 28219758592.0000\n",
            "Epoch 241/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260794335232.0000 - mae: 127226.2969 - mse: 260794335232.0000 - val_loss: 28205617152.0000 - val_mae: 113027.2891 - val_mse: 28205617152.0000\n",
            "Epoch 242/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260769234944.0000 - mae: 127190.8359 - mse: 260769234944.0000 - val_loss: 28195076096.0000 - val_mae: 112984.8438 - val_mse: 28195076096.0000\n",
            "Epoch 243/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260746412032.0000 - mae: 127160.9297 - mse: 260746412032.0000 - val_loss: 28184291328.0000 - val_mae: 112941.3750 - val_mse: 28184291328.0000\n",
            "Epoch 244/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260723736576.0000 - mae: 127130.3047 - mse: 260723736576.0000 - val_loss: 28172025856.0000 - val_mae: 112891.9688 - val_mse: 28172025856.0000\n",
            "Epoch 245/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260700143616.0000 - mae: 127098.6953 - mse: 260700143616.0000 - val_loss: 28158603264.0000 - val_mae: 112839.9531 - val_mse: 28158603264.0000\n",
            "Epoch 246/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260676861952.0000 - mae: 127067.2188 - mse: 260676861952.0000 - val_loss: 28148291584.0000 - val_mae: 112795.5703 - val_mse: 28148291584.0000\n",
            "Epoch 247/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260655153152.0000 - mae: 127042.3203 - mse: 260655153152.0000 - val_loss: 28136531968.0000 - val_mae: 112750.9766 - val_mse: 28136531968.0000\n",
            "Epoch 248/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260633427968.0000 - mae: 127015.2891 - mse: 260633427968.0000 - val_loss: 28124231680.0000 - val_mae: 112700.1328 - val_mse: 28124231680.0000\n",
            "Epoch 249/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260610637824.0000 - mae: 126987.5156 - mse: 260610637824.0000 - val_loss: 28114448384.0000 - val_mae: 112661.0312 - val_mse: 28114448384.0000\n",
            "Epoch 250/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260589617152.0000 - mae: 126962.5234 - mse: 260589617152.0000 - val_loss: 28100966400.0000 - val_mae: 112610.8594 - val_mse: 28100966400.0000\n",
            "Epoch 251/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260566908928.0000 - mae: 126933.1641 - mse: 260566908928.0000 - val_loss: 28090920960.0000 - val_mae: 112571.5078 - val_mse: 28090920960.0000\n",
            "Epoch 252/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260546625536.0000 - mae: 126911.3750 - mse: 260546625536.0000 - val_loss: 28079581184.0000 - val_mae: 112528.9219 - val_mse: 28079581184.0000\n",
            "Epoch 253/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260525834240.0000 - mae: 126886.9219 - mse: 260525834240.0000 - val_loss: 28069111808.0000 - val_mae: 112485.0000 - val_mse: 28069111808.0000\n",
            "Epoch 254/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260504387584.0000 - mae: 126860.5781 - mse: 260504387584.0000 - val_loss: 28057868288.0000 - val_mae: 112443.7891 - val_mse: 28057868288.0000\n",
            "Epoch 255/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260484300800.0000 - mae: 126836.7969 - mse: 260484300800.0000 - val_loss: 28049993728.0000 - val_mae: 112408.9219 - val_mse: 28049993728.0000\n",
            "Epoch 256/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260465033216.0000 - mae: 126817.1328 - mse: 260465033216.0000 - val_loss: 28039700480.0000 - val_mae: 112367.0312 - val_mse: 28039700480.0000\n",
            "Epoch 257/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260444012544.0000 - mae: 126792.2344 - mse: 260444012544.0000 - val_loss: 28031518720.0000 - val_mae: 112330.1562 - val_mse: 28031518720.0000\n",
            "Epoch 258/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260424712192.0000 - mae: 126772.3984 - mse: 260424712192.0000 - val_loss: 28021856256.0000 - val_mae: 112289.3438 - val_mse: 28021856256.0000\n",
            "Epoch 259/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260404445184.0000 - mae: 126750.4219 - mse: 260404445184.0000 - val_loss: 28011501568.0000 - val_mae: 112247.1328 - val_mse: 28011501568.0000\n",
            "Epoch 260/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260383801344.0000 - mae: 126728.3047 - mse: 260383801344.0000 - val_loss: 28004454400.0000 - val_mae: 112216.7812 - val_mse: 28004454400.0000\n",
            "Epoch 261/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260365877248.0000 - mae: 126712.7656 - mse: 260365877248.0000 - val_loss: 27993481216.0000 - val_mae: 112173.6172 - val_mse: 27993481216.0000\n",
            "Epoch 262/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260345643008.0000 - mae: 126692.6562 - mse: 260345643008.0000 - val_loss: 27985401856.0000 - val_mae: 112140.3281 - val_mse: 27985401856.0000\n",
            "Epoch 263/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260327899136.0000 - mae: 126678.3516 - mse: 260327899136.0000 - val_loss: 27974166528.0000 - val_mae: 112101.3594 - val_mse: 27974166528.0000\n",
            "Epoch 264/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260308434944.0000 - mae: 126658.8594 - mse: 260308434944.0000 - val_loss: 27964905472.0000 - val_mae: 112065.8125 - val_mse: 27964905472.0000\n",
            "Epoch 265/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260289839104.0000 - mae: 126642.5391 - mse: 260289839104.0000 - val_loss: 27958996992.0000 - val_mae: 112040.5703 - val_mse: 27958996992.0000\n",
            "Epoch 266/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260273094656.0000 - mae: 126632.0156 - mse: 260273094656.0000 - val_loss: 27950804992.0000 - val_mae: 112009.5312 - val_mse: 27950804992.0000\n",
            "Epoch 267/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260255367168.0000 - mae: 126617.5000 - mse: 260255367168.0000 - val_loss: 27942383616.0000 - val_mae: 111977.9922 - val_mse: 27942383616.0000\n",
            "Epoch 268/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260237705216.0000 - mae: 126602.6797 - mse: 260237705216.0000 - val_loss: 27937218560.0000 - val_mae: 111955.2500 - val_mse: 27937218560.0000\n",
            "Epoch 269/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260221255680.0000 - mae: 126592.1250 - mse: 260221255680.0000 - val_loss: 27929610240.0000 - val_mae: 111930.1172 - val_mse: 27929610240.0000\n",
            "Epoch 270/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260204232704.0000 - mae: 126577.9062 - mse: 260204232704.0000 - val_loss: 27920283648.0000 - val_mae: 111899.9766 - val_mse: 27920279552.0000\n",
            "Epoch 271/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 260186226688.0000 - mae: 126559.7812 - mse: 260186226688.0000 - val_loss: 27914340352.0000 - val_mae: 111878.5938 - val_mse: 27914340352.0000\n",
            "Epoch 272/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 260170268672.0000 - mae: 126549.1094 - mse: 260170268672.0000 - val_loss: 27906150400.0000 - val_mae: 111855.1562 - val_mse: 27906150400.0000\n",
            "Epoch 273/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260153376768.0000 - mae: 126536.2500 - mse: 260153376768.0000 - val_loss: 27900579840.0000 - val_mae: 111839.2500 - val_mse: 27900579840.0000\n",
            "Epoch 274/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260138942464.0000 - mae: 126528.1094 - mse: 260138942464.0000 - val_loss: 27895095296.0000 - val_mae: 111819.6172 - val_mse: 27895095296.0000\n",
            "Epoch 275/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260122984448.0000 - mae: 126516.6406 - mse: 260122984448.0000 - val_loss: 27891591168.0000 - val_mae: 111807.1641 - val_mse: 27891591168.0000\n",
            "Epoch 276/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260108976128.0000 - mae: 126510.2109 - mse: 260108976128.0000 - val_loss: 27886002176.0000 - val_mae: 111787.5859 - val_mse: 27886002176.0000\n",
            "Epoch 277/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260093067264.0000 - mae: 126497.6875 - mse: 260093067264.0000 - val_loss: 27879694336.0000 - val_mae: 111772.3750 - val_mse: 27879694336.0000\n",
            "Epoch 278/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 260077895680.0000 - mae: 126487.9922 - mse: 260077895680.0000 - val_loss: 27877050368.0000 - val_mae: 111759.8359 - val_mse: 27877050368.0000\n",
            "Epoch 279/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260063559680.0000 - mae: 126481.1562 - mse: 260063559680.0000 - val_loss: 27869798400.0000 - val_mae: 111739.1016 - val_mse: 27869798400.0000\n",
            "Epoch 280/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 260047405056.0000 - mae: 126468.6641 - mse: 260047405056.0000 - val_loss: 27864739840.0000 - val_mae: 111720.7578 - val_mse: 27864739840.0000\n",
            "Epoch 281/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 260032200704.0000 - mae: 126459.3438 - mse: 260032200704.0000 - val_loss: 27859800064.0000 - val_mae: 111703.7656 - val_mse: 27859800064.0000\n",
            "Epoch 282/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 260016586752.0000 - mae: 126447.9297 - mse: 260016586752.0000 - val_loss: 27853846528.0000 - val_mae: 111683.3281 - val_mse: 27853846528.0000\n",
            "Epoch 283/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 260000055296.0000 - mae: 126436.3672 - mse: 260000055296.0000 - val_loss: 27852664832.0000 - val_mae: 111680.5938 - val_mse: 27852664832.0000\n",
            "Epoch 284/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259986636800.0000 - mae: 126432.3828 - mse: 259986636800.0000 - val_loss: 27848804352.0000 - val_mae: 111663.8672 - val_mse: 27848804352.0000\n",
            "Epoch 285/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259970973696.0000 - mae: 126421.9609 - mse: 259970973696.0000 - val_loss: 27845564416.0000 - val_mae: 111650.2422 - val_mse: 27845564416.0000\n",
            "Epoch 286/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259955638272.0000 - mae: 126411.4766 - mse: 259955638272.0000 - val_loss: 27842308096.0000 - val_mae: 111636.9922 - val_mse: 27842308096.0000\n",
            "Epoch 287/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259940597760.0000 - mae: 126401.2656 - mse: 259940597760.0000 - val_loss: 27836981248.0000 - val_mae: 111615.1953 - val_mse: 27836981248.0000\n",
            "Epoch 288/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259924541440.0000 - mae: 126389.4609 - mse: 259924541440.0000 - val_loss: 27836405760.0000 - val_mae: 111608.4609 - val_mse: 27836405760.0000\n",
            "Epoch 289/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259910811648.0000 - mae: 126385.1797 - mse: 259910811648.0000 - val_loss: 27833677824.0000 - val_mae: 111595.9531 - val_mse: 27833677824.0000\n",
            "Epoch 290/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259897147392.0000 - mae: 126378.0625 - mse: 259897147392.0000 - val_loss: 27829745664.0000 - val_mae: 111577.5391 - val_mse: 27829745664.0000\n",
            "Epoch 291/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259882696704.0000 - mae: 126368.4531 - mse: 259882696704.0000 - val_loss: 27827238912.0000 - val_mae: 111564.6562 - val_mse: 27827238912.0000\n",
            "Epoch 292/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259868508160.0000 - mae: 126359.3281 - mse: 259868508160.0000 - val_loss: 27823757312.0000 - val_mae: 111547.0312 - val_mse: 27823757312.0000\n",
            "Epoch 293/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259853549568.0000 - mae: 126347.7188 - mse: 259853549568.0000 - val_loss: 27821142016.0000 - val_mae: 111531.7031 - val_mse: 27821142016.0000\n",
            "Epoch 294/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259838803968.0000 - mae: 126337.2578 - mse: 259838803968.0000 - val_loss: 27820498944.0000 - val_mae: 111525.1406 - val_mse: 27820498944.0000\n",
            "Epoch 295/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 259825582080.0000 - mae: 126329.6797 - mse: 259825582080.0000 - val_loss: 27817281536.0000 - val_mae: 111504.9375 - val_mse: 27817281536.0000\n",
            "Epoch 296/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259810131968.0000 - mae: 126316.0312 - mse: 259810131968.0000 - val_loss: 27815385088.0000 - val_mae: 111490.6953 - val_mse: 27815385088.0000\n",
            "Epoch 297/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259795533824.0000 - mae: 126304.7188 - mse: 259795533824.0000 - val_loss: 27812675584.0000 - val_mae: 111474.2422 - val_mse: 27812675584.0000\n",
            "Epoch 298/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259781197824.0000 - mae: 126294.0078 - mse: 259781197824.0000 - val_loss: 27811905536.0000 - val_mae: 111461.9375 - val_mse: 27811905536.0000\n",
            "Epoch 299/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259767091200.0000 - mae: 126285.5000 - mse: 259767091200.0000 - val_loss: 27807272960.0000 - val_mae: 111445.0547 - val_mse: 27807272960.0000\n",
            "Epoch 300/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259752886272.0000 - mae: 126275.2891 - mse: 259752886272.0000 - val_loss: 27806519296.0000 - val_mae: 111432.1406 - val_mse: 27806519296.0000\n",
            "Epoch 301/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259739303936.0000 - mae: 126267.5547 - mse: 259739303936.0000 - val_loss: 27805222912.0000 - val_mae: 111419.8438 - val_mse: 27805222912.0000\n",
            "Epoch 302/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259725819904.0000 - mae: 126261.6250 - mse: 259725819904.0000 - val_loss: 27803170816.0000 - val_mae: 111409.6250 - val_mse: 27803170816.0000\n",
            "Epoch 303/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259713187840.0000 - mae: 126256.8906 - mse: 259713187840.0000 - val_loss: 27802296320.0000 - val_mae: 111398.2891 - val_mse: 27802296320.0000\n",
            "Epoch 304/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259700817920.0000 - mae: 126251.1172 - mse: 259700817920.0000 - val_loss: 27800158208.0000 - val_mae: 111385.1875 - val_mse: 27800158208.0000\n",
            "Epoch 305/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259687874560.0000 - mae: 126246.9844 - mse: 259687874560.0000 - val_loss: 27798474752.0000 - val_mae: 111372.0469 - val_mse: 27798474752.0000\n",
            "Epoch 306/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259675308032.0000 - mae: 126244.2422 - mse: 259675308032.0000 - val_loss: 27796129792.0000 - val_mae: 111357.8203 - val_mse: 27796129792.0000\n",
            "Epoch 307/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259662430208.0000 - mae: 126240.8828 - mse: 259662430208.0000 - val_loss: 27795609600.0000 - val_mae: 111348.1172 - val_mse: 27795609600.0000\n",
            "Epoch 308/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259650306048.0000 - mae: 126239.9688 - mse: 259650306048.0000 - val_loss: 27794796544.0000 - val_mae: 111338.5859 - val_mse: 27794796544.0000\n",
            "Epoch 309/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259637624832.0000 - mae: 126238.5156 - mse: 259637624832.0000 - val_loss: 27792803840.0000 - val_mae: 111326.5469 - val_mse: 27792803840.0000\n",
            "Epoch 310/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 259625172992.0000 - mae: 126239.6797 - mse: 259625172992.0000 - val_loss: 27790882816.0000 - val_mae: 111314.9375 - val_mse: 27790882816.0000\n",
            "Epoch 311/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259612491776.0000 - mae: 126240.1250 - mse: 259612491776.0000 - val_loss: 27789699072.0000 - val_mae: 111307.3203 - val_mse: 27789699072.0000\n",
            "Epoch 312/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259600416768.0000 - mae: 126240.8203 - mse: 259600416768.0000 - val_loss: 27788511232.0000 - val_mae: 111298.9141 - val_mse: 27788511232.0000\n",
            "Epoch 313/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259588882432.0000 - mae: 126243.9531 - mse: 259588882432.0000 - val_loss: 27788568576.0000 - val_mae: 111292.9922 - val_mse: 27788568576.0000\n",
            "Epoch 314/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 259577544704.0000 - mae: 126245.9531 - mse: 259577544704.0000 - val_loss: 27786979328.0000 - val_mae: 111284.7109 - val_mse: 27786979328.0000\n",
            "Epoch 315/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259564748800.0000 - mae: 126244.3125 - mse: 259564748800.0000 - val_loss: 27788406784.0000 - val_mae: 111286.5312 - val_mse: 27788406784.0000\n",
            "Epoch 316/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259554983936.0000 - mae: 126250.1406 - mse: 259554983936.0000 - val_loss: 27786074112.0000 - val_mae: 111272.9531 - val_mse: 27786074112.0000\n",
            "Epoch 317/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259542384640.0000 - mae: 126248.3516 - mse: 259542384640.0000 - val_loss: 27786123264.0000 - val_mae: 111268.3750 - val_mse: 27786123264.0000\n",
            "Epoch 318/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259531046912.0000 - mae: 126249.3047 - mse: 259531046912.0000 - val_loss: 27784581120.0000 - val_mae: 111260.5938 - val_mse: 27784581120.0000\n",
            "Epoch 319/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259520708608.0000 - mae: 126252.9453 - mse: 259520708608.0000 - val_loss: 27782948864.0000 - val_mae: 111253.0625 - val_mse: 27782948864.0000\n",
            "Epoch 320/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259509452800.0000 - mae: 126254.7812 - mse: 259509452800.0000 - val_loss: 27784622080.0000 - val_mae: 111253.1016 - val_mse: 27784622080.0000\n",
            "Epoch 321/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 259499966464.0000 - mae: 126258.5469 - mse: 259499966464.0000 - val_loss: 27781666816.0000 - val_mae: 111236.7500 - val_mse: 27781666816.0000\n",
            "Epoch 322/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 259488186368.0000 - mae: 126255.7812 - mse: 259488186368.0000 - val_loss: 27779549184.0000 - val_mae: 111227.4844 - val_mse: 27779549184.0000\n",
            "Epoch 323/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 259476684800.0000 - mae: 126257.7500 - mse: 259476684800.0000 - val_loss: 27779731456.0000 - val_mae: 111222.9922 - val_mse: 27779731456.0000\n",
            "Epoch 324/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 259466543104.0000 - mae: 126261.2578 - mse: 259466543104.0000 - val_loss: 27778217984.0000 - val_mae: 111212.6250 - val_mse: 27778217984.0000\n",
            "Epoch 325/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 259455811584.0000 - mae: 126262.4844 - mse: 259455811584.0000 - val_loss: 27777167360.0000 - val_mae: 111206.9531 - val_mse: 27777167360.0000\n",
            "Epoch 326/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 259444768768.0000 - mae: 126264.7266 - mse: 259444768768.0000 - val_loss: 27774179328.0000 - val_mae: 111195.1953 - val_mse: 27774179328.0000\n",
            "Epoch 327/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259434184704.0000 - mae: 126267.6328 - mse: 259434184704.0000 - val_loss: 27771961344.0000 - val_mae: 111180.3438 - val_mse: 27771961344.0000\n",
            "Epoch 328/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259422437376.0000 - mae: 126265.3281 - mse: 259422437376.0000 - val_loss: 27773566976.0000 - val_mae: 111177.8906 - val_mse: 27773566976.0000\n",
            "Epoch 329/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 259411574784.0000 - mae: 126268.3281 - mse: 259411574784.0000 - val_loss: 27771152384.0000 - val_mae: 111166.7266 - val_mse: 27771152384.0000\n",
            "Epoch 330/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259400302592.0000 - mae: 126269.5469 - mse: 259400302592.0000 - val_loss: 27769430016.0000 - val_mae: 111156.2500 - val_mse: 27769430016.0000\n",
            "Epoch 331/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259390242816.0000 - mae: 126272.7500 - mse: 259390242816.0000 - val_loss: 27769528320.0000 - val_mae: 111144.5391 - val_mse: 27769528320.0000\n",
            "Epoch 332/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259378397184.0000 - mae: 126271.0234 - mse: 259378397184.0000 - val_loss: 27769841664.0000 - val_mae: 111136.0469 - val_mse: 27769841664.0000\n",
            "Epoch 333/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259367813120.0000 - mae: 126270.9297 - mse: 259367813120.0000 - val_loss: 27771396096.0000 - val_mae: 111131.3203 - val_mse: 27771396096.0000\n",
            "Epoch 334/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259357589504.0000 - mae: 126272.6172 - mse: 259357589504.0000 - val_loss: 27769792512.0000 - val_mae: 111114.6016 - val_mse: 27769792512.0000\n",
            "Epoch 335/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259346644992.0000 - mae: 126272.1016 - mse: 259346644992.0000 - val_loss: 27770988544.0000 - val_mae: 111109.0000 - val_mse: 27770988544.0000\n",
            "Epoch 336/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259335192576.0000 - mae: 126269.2422 - mse: 259335192576.0000 - val_loss: 27772018688.0000 - val_mae: 111100.6250 - val_mse: 27772018688.0000\n",
            "Epoch 337/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259325706240.0000 - mae: 126270.0078 - mse: 259325706240.0000 - val_loss: 27771873280.0000 - val_mae: 111088.4297 - val_mse: 27771873280.0000\n",
            "Epoch 338/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 259312795648.0000 - mae: 126265.5938 - mse: 259312795648.0000 - val_loss: 27774078976.0000 - val_mae: 111086.7500 - val_mse: 27774078976.0000\n",
            "Epoch 339/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259303129088.0000 - mae: 126267.2344 - mse: 259303129088.0000 - val_loss: 27771426816.0000 - val_mae: 111073.1016 - val_mse: 27771426816.0000\n",
            "Epoch 340/1000\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 259291054080.0000 - mae: 126267.6797 - mse: 259291054080.0000 - val_loss: 27775600640.0000 - val_mae: 111072.8594 - val_mse: 27775600640.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['mae'], label='Training Loss', )\n",
        "plt.plot(history.history['val_mae'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "eZYIaccC38GL",
        "outputId": "35e018a5-6c7c-4ab9-aaf2-a6404511c0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_mae, test_mse = model.evaluate(X_test, y_test)\n",
        "print(f\"Test MAE: {test_mae}\")\n",
        "print(f\"Test MSE: {test_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TudTwW2g5I0P",
        "outputId": "1e4141a2-d504-4096-f5ca-09d242bd6c18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42960543744.0000 - mae: 123659.7188 - mse: 42960543744.0000 \n",
            "Test MAE: 44126265344.0\n",
            "Test MSE: 123281.4765625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export the Trained Model"
      ],
      "metadata": {
        "id": "HivKk96L-JRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained model is saved to a file using Python's `pickle` module. This allows the model to be persisted and reloaded later without the need to retrain.\n",
        "\n",
        "* `trained_model.pkl` specifies the filename for the saved model.\n",
        "* `wb` opens the file in binary write mode.\n",
        "* `pickle.dump(model, file)` serializes the trained model object and writes it to the file.\n",
        "\n",
        "This process ensures that the model's architecture and learned weights are stored in a file, making it convenient to deploy or share the model in the future."
      ],
      "metadata": {
        "id": "TRRZuYZ-RY3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trained_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "FwZrUE0TmW3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Trained Model"
      ],
      "metadata": {
        "id": "pzLTt6PT-R4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to load the previously saved model from a file, we can use Python's `pickle` module.\n",
        "\n",
        "* `/content/trained_model.pkl` specifies the path to the file containing the saved model.\n",
        "* `rb` opens the file in binary read mode.\n",
        "* `pickle.load(file)` deserializes the model object from the file, restoring the trained model for future use.\n",
        "\n",
        "This step allows you to reload the model for evaluation, inference, or further training, without needing to retrain it from scratch."
      ],
      "metadata": {
        "id": "94sVaArFRxpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/trained_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)"
      ],
      "metadata": {
        "id": "UH7b65AVUVhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of a New Data Point using the Trained Model"
      ],
      "metadata": {
        "id": "AjsoYzSH-WRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A new data point is prepared for prediction by creating a dictionary with the required features, which is then converted into a DataFrame. The data is processed similarly to the training data:\n",
        "\n",
        "* `create_new_features()` adds new features such as house_age and years_since_renovation.\n",
        "* Bucketization and normalization: The features are bucketized and normalized according to the preprocessing steps applied to the training data."
      ],
      "metadata": {
        "id": "9m2HdGofSTj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_pred = {key:0 for key in X_train.columns}\n",
        "\n",
        "new_pred['date'] = pd.to_datetime('2014-07-10')\n",
        "new_pred['bedrooms'] = 5\n",
        "new_pred['bathrooms'] = 3\n",
        "new_pred['sqft_living'] = 10000\n",
        "new_pred['sqft_lot'] = 1000\n",
        "new_pred['floors'] = 2\n",
        "new_pred['waterfront'] = 1\n",
        "new_pred['view'] = 3\n",
        "new_pred['condition'] = 5\n",
        "new_pred['sqft_above'] = 500\n",
        "new_pred['sqft_basement'] = 500\n",
        "new_pred['yr_built'] = 2000\n",
        "new_pred['yr_renovated'] = 2012\n",
        "new_pred['city_Bellevue'] = 1\n",
        "\n",
        "new_pred = pd.DataFrame([new_pred])\n",
        "new_pred = create_new_features(new_pred)\n",
        "for col, size in bucket_sizes.items():\n",
        "    new_pred[col] = bucketize(new_pred, col, size)\n",
        "for col in numerical_features:\n",
        "    new_pred[col] = normalize(new_pred, col)"
      ],
      "metadata": {
        "id": "lhFJxWWOgUB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processed data is fed into the trained model to predict the house price. The `predict` method generates the predicted price, which is then printed in a formatted manner.\n",
        "\n",
        "This process demonstrates how to make predictions on new data using a previously trained and saved model."
      ],
      "metadata": {
        "id": "8UnOakGfSd8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the price\n",
        "predicted_price = loaded_model.predict(new_pred)\n",
        "\n",
        "# Output the predicted price\n",
        "print(f\"Predicted house price: ${predicted_price[0][0]:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTfXTCI56wJu",
        "outputId": "8af48e11-81e0-41fb-b879-8aad6ebc70b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Predicted house price: $2,399,025.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing a Web Application Based on the Trained Model"
      ],
      "metadata": {
        "id": "KadRj8JsYiGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we develop a web application using `streamlit`. This interface will allow users to input features and then use the trained model to predict house prices. [Here](https://huggingface.co/spaces/RMHalak/house-pricing-v1/blob/main/app.py) you can find the development of the web application."
      ],
      "metadata": {
        "id": "QmRPe3TzYqhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploying the Application on Hugging Face Spaces"
      ],
      "metadata": {
        "id": "dy6i2qqsZdbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we can deploy the application by hosting it on a cloud service such as [Hugging Face Spaces](https://huggingface.co/spaces). This makes the model accessible via a web browser for other users."
      ],
      "metadata": {
        "id": "HWTl3n13ZKcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "html_code = '''\n",
        "<iframe\n",
        "    src=\"https://rmhalak-house-pricing-v1.hf.space/\"\n",
        "    frameborder=\"0\"\n",
        "    width=\"1200\"\n",
        "    height=\"750\"\n",
        "></iframe>\n",
        "'''\n",
        "\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "GCM1ChS2aEs6",
        "outputId": "9a61d783-6b40-4884-a089-a302755b0221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe\n",
              "    src=\"https://rmhalak-house-pricing-v1.hf.space/\"\n",
              "    frameborder=\"0\"\n",
              "    width=\"1200\"\n",
              "    height=\"750\"\n",
              "></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}